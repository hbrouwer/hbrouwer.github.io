<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Mac OS X (vers 31 October 2006 - Apple Inc. build 6349), see www.w3.org" />
  <meta name="generator" content="vim" />
  <meta http-equiv="Content-Type" content=
  "text/html; charset=us-ascii" />

  <title>Connectionist Language Processing</title>
  <style type="text/css" title="Preferred Stylesheet" media=
  "screen">
/*<![CDATA[*/ @import url("../../stylesheet.css"); /*]]>*/
  </style>
</head>

<body>
  <div class="content">
    <div class="header">
      <span class="title">CLP21</span> <span class=
      "name">Connectionist Language Processing (Summer 2021)</span>
    </div>

    <p class="nav_links"><a href="index.html#synopsis">Synopsis</a>
    | <a href="index.html#overview">Overview</a> | <a href=
    "index.html#schedule">Schedule</a> | <a href=
    "index.html#literature">Literature</a></p>

    <h1 id="synopsis">synopsis</h1>
    <hr class="section" />

    <p class="item">This course will examine neurocomputational (or
    connectionist) models of human language processing. We will
    start from biological neurons, and show how their processing
    behaviour can be modelled mathematically. The resulting
    artificial neurons will then be wired together to form
    artificial neural networks, and we will discuss how such
    networks can be applied to build neurocomputational models of
    language learning and language processing. It will be shown
    that such models effectively all share the same computational
    principles, and that any differences in their behaviour is
    driven by differences in the representations that they process
    and construct. Near the end of the course, we will use the
    accumulated knowledge to construct a psychologically plausible
    neurocomputational model of incremental (word-by-word) language
    comprehension that constructs a rich utterance representation
    beyond a simple syntactic derivation or semantic formula.</p>

    <h1 id="overview">course overview</h1>
    <hr class="section" />

    <p class="item"><a href=
    "https://www.lsf.uni-saarland.de/qisserver/rds?state=verpublish&amp;status=init&amp;vmfile=no&amp;publishid=127736&amp;moduleCall=webInfo&amp;publishConfFile=webInfo&amp;publishSubDir=veranstaltung">
    Connectionist Language Processing</a> is a course taught in the
    <a href="https://www.lst.uni-saarland.de/">Department of
    Language Science and Technology</a> at <a href=
    "https://www.uni-saarland.de/">Saarland University</a>. It is
    open for master-level students.</p>

    <p class="item"><strong>Lecturer:</strong> <a href=
    "https://hbrouwer.github.io/">Harm Brouwer</a> &lt;<span style=
    "font-family: courier"><a href=
    "mailto:me[at]hbrouwer.eu">me[at]hbrouwer.eu</a></span>&gt;<br />

    <strong>TA:</strong> <a href=
    "https://caurnhammer.github.io/">Christoph Aurnhammer</a>
    &lt;<span style="font-family: courier"><a href=
    "mailto:aurnhammer[at]coli.uni-saarland.de">aurnhammer[at]coli.uni-saarland.de</a></span>&gt;</p>

    <p class="item"><strong>Time:</strong> Tuesday 14:15-15:45;
    Thursday 14:15-15:45<br />
    <strong>Place:</strong> Online (Microsoft Teams)<br />
    <strong>Start:</strong> 20.04.21<br /></p>

    <p class="item"><strong>Exam:</strong> Tuesday, July 20,
    14:00-16.00<br />
    <strong>Credits:</strong> 6 CP</p>

    <p class="item"><strong>Registration:</strong> <a href=
    "mailto:me[at]hbrouwer.eu?subject=CLP21%20enrolment">Send me an
    email to enrol for the course</a><br /></p>

    <p class="item"><strong>Format and Requirements:</strong></p>

    <ul>
      <li>The course will be taught online with a strong focus on
      self-study;</li>

      <li>Slides and suggested reading materials will be made
      available each week, and lecture sessions will serve to
      discuss these in an interactive Q&amp;A style manner;</li>

      <li>Tutorials will be made available after each lecture, and
      you should try to complete them as far as possible before the
      corresponding tutorial session, where we will interactively
      discuss them in a Q&amp;A style manner;</li>

      <li>Completed tutorial sheets should be handed in before the
      next lecture;</li>

      <li>You are expected to attend all lecture and tutorial
      sessions, and to complete all tutorial sheets.</li>
    </ul>

    <h1 id="schedule">schedule</h1>
    <hr class="section" />

    <p class="item">This is the <strong>provisional</strong> course
    schedule. See <a href="index.html#literature">below</a> for
    suggested background literature.</p>

    <table class="schedule" summary="Schedule">
      <tr class="schedule">
        <th class="schedule">Date</th>

        <th class="schedule">Topic</th>
      </tr>

      <tr class="schedule">
        <td class="schedule">20.04.21</td>

        <td class="schedule"><strong>Lecture 1:</strong>
        Introduction to Connectionism and the Brain</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">22.04.21</td>

        <td class="schedule"><strong>Tutorial 1:</strong>
        Introduction to Neural Networks in MESH</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">27.04.21</td>

        <td class="schedule"><strong>Lecture 2:</strong> A Primer
        on Linear Algebra</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">29.04.21</td>

        <td class="schedule"><strong>Lecture 3:</strong> Learning
        in Single-layer Networks</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">04.05.21</td>

        <td class="schedule"><strong>Lecture 4:</strong> Training
        Multi-layer Networks</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">06.05.21</td>

        <td class="schedule"><strong>Tutorial 2:</strong> Training
        Multi-layer Networks</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">11.05.21</td>

        <td class="schedule"><strong>Lecture 5:</strong> Reading
        Aloud</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">13.05.21</td>

        <td class="schedule">(no class: <a href=
        "https://de.wikipedia.org/wiki/Christi_Himmelfahrt">Christi
        Himmelfahrt</a>)</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">18.05.21</td>

        <td class="schedule"><strong>Tutorial 3:</strong> Reading
        Aloud</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">20.05.21</td>

        <td class="schedule"><strong>Lecture 6:</strong> English
        Past Tense</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">25.05.21</td>

        <td class="schedule"><strong>Tutorial 4:</strong> English
        Past Tense</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">27.05.21</td>

        <td class="schedule"><strong>Lecture 7:</strong> Simple
        Recurrent Networks I</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">01.06.21</td>

        <td class="schedule"><strong>Lecture 8:</strong> Simple
        Recurrent Networks II</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">03.06.21</td>

        <td class="schedule">(no class: <a href=
        "https://de.wikipedia.org/wiki/Fronleichnam">Fronleichnam</a>)</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">08.06.21</td>

        <td class="schedule"><strong>Lecture 9:</strong> Recurrent
        Neural Networks as Models of Sentence Processing
        (<strong>Christoph Aurnhammer</strong>)</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">10.06.21</td>

        <td class="schedule"><strong>Tutorial 5:</strong> Simple
        Recurrent Networks</td>
      </tr>
      
      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">15.06.21</td>

        <td class="schedule"><strong>Lecure 10:</strong> Modeling
        the Electrophysiology of Language Comprehension</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">17.06.21</td>

        <td class="schedule"><strong>Lecture 11:</strong> Situation
        Modeling using Microworlds</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">22.06.21</td>

        <td class="schedule"><strong>Tutorial 6:</strong>
        Expectation-based Comprehension I</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">24.06.21</td>

        <td class="schedule"><strong>Lecture 12:</strong> Modeling
        the Neurobehavioral Correlates of Comprehension-centric
        Surprisal</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">29.06.21</td>

        <td class="schedule"><strong>Tutorial 7:</strong>
        Expectation-based Comprehension II</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">01.07.21</td>

        <td class="schedule"><strong>Tutorial Completion
        Session</strong></td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">06.07.21</td>

        <td class="schedule"><strong>Lecture 13:</strong> Course
        Summary</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">08.07.21</td>

        <td class="schedule"><strong>Q&amp;A</strong></td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">13.07.21</td>

        <td class="schedule">(no class)</td>
      </tr>

      <tr class="schedule">
        <td class="schedule">15.07.21</td>

        <td class="schedule">(no class)</td>
      </tr>

      <tr>
        <td>
          <hr class="section" />
        </td>

        <td>
          <hr class="section" />
        </td>
      </tr>

      <tr class="schedule">
        <td class="schedule">20.07.21</td>

        <td class="schedule"><strong>Exam</strong></td>
      </tr>
    </table>

    <h1 id="literature">suggested literature</h1>
    <hr class="section" />
    This is a inexhaustive list of suggested literature organized
    by lecture. For each lecture, the list is ordered in terms of
    the relevance/closeness of the articles to the material
    presented in that lecture. Articles marked with an asterisk (*)
    are (co-)authored by me.

    <h2>20.04.21</h2>

    <ol type="1">
      <li>Plunkett K, and Elman J. (1997). <i>Exercises in
      rethinking innateness: A Handbook for Connectionist
      Simulations</i>. Cambridge, MA: MIT Press. <strong>Chapter
      2.</strong></li>
    </ol>

    <h2>27.04.21</h2>

    <ol type="1">
      <li>Jordan, M. I. (1986). An introduction to linear algebra
      in parallel distributed processing. In: Rumelhart, D. E., and
      McClelland, J. L., et al. <i>Parallel Distributed Processing,
      Vol. 1</i>, pp. 365-422.</li>
    </ol>

    <h2>29.04.21</h2>

    <ol type="1">
      <li>Plunkett K, and Elman J. (1997). <i>Exercises in
      rethinking innateness: A Handbook for Connectionist
      Simulations</i>. Cambridge, MA: MIT Press. <strong>Chapter
      1.</strong></li>

      <li>*Brouwer, H. (2014). <i>The Electrophysiology of Language
      Comprehension: A Neurocomputational Model</i>. PhD thesis.
      University of Groningen, Groningen, The Netherlands.
      <strong>Appendix A (up to and including A.2.2)</strong></li>
    </ol>

    <h2>04.05.21</h2>

    <ol type="1">
      <li>Plunkett K, and Elman J. (1997). <i>Exercises in
      rethinking innateness: A Handbook for Connectionist
      Simulations</i>. Cambridge, MA: MIT Press. <strong>Chapters 1
      and 4.</strong></li>

      <li>*Brouwer, H. (2014). <i>The Electrophysiology of Language
      Comprehension: A Neurocomputational Model</i>. PhD thesis.
      University of Groningen, Groningen, The Netherlands.
      <strong>Appendix A (up to and including A.2.3)</strong></li>
    </ol>

    <h2>11.05.21</h2>

    <ol type="1">
      <li>Plaut, D. C., McClelland, J. L., Seidenberg, M. S., and
      Patterson, K. (1996). Understanding normal and impaired word
      reading: Computational principles in quasi-regular domains.
      <i>Psychological Review, 103</i>, pp. 56-115. <strong>Pages
      1-19 (up to and including Summary)</strong></li>
    </ol>

    <h2>20.05.21</h2>

    <ol type="1">
      <li>Plunkett K, and Marchman V. A. (1991). U-shaped learning
      and frequency effects in a multi-layered perceptron:
      Implications for child language acquisition. <i>Cognition,
      38</i>(1). pp. 43-102. <strong>Pages 1-15 (up to and
      including 3.1)</strong></li>
    </ol>

    <h2>27.05.21</h2>

    <ol type="1">
      <li>Elman, J. (1990). Finding structure in time. <i>Cognitive
      Science, 14</i>, pp. 179-211.</li>

      <li>*Brouwer, H. (2014). <i>The Electrophysiology of Language
      Comprehension: A Neurocomputational Model</i>. PhD thesis.
      University of Groningen, Groningen, The Netherlands.
      <strong>Appendix A (up to and including A.2.4)</strong></li>
    </ol>

    <h2>01.06.21</h2>

    <ol>
      <li>Elman, J. (1991). Distributed representations, simple
      recurrent networks, and grammatical structure. <i>Machine
      Learning, 7</i>, pp. 195-225.</li>

      <li>Elman, J. (1993). Learning and development in neural
      networks: the importance of starting small <i>Cognition,
      48</i>, pp. 71-99.</li>
    </ol>

    <h2>15.06.21</h2>

    <ol type="1">
      <li>*Brouwer, H., Crocker, M. W., Venhuizen, N. J., and
      Hoeks, J. C. J. (2017). A Neurocomputational Model of the
      N400 and the P600 in Language Processing. <i>Cognitive
      Science, 41</i>(S6), pp. 1318-1352.</li>
    </ol>

    <h2>17.06.21</h2>

    <ol type="1">
      <li>*Venhuizen, N. J., Crocker, M. W., and Brouwer, H.
      (2019). Expectation-based Comprehension: Modeling the
      interaction of world knowledge and linguistic experience.
      <i>Discourse Processes, 56</i>:3, pp. 229-255. <strong>Pages
      1-19 (up to and including Equation 6)</strong></li>
    </ol>

    <h2>24.06.21</h2>

    <ol type="1">
      <li>*Brouwer, H., Delogu, F, Venhuizen, N. J., and Crocker,
      M. W. (2021). Neurobehavioral Correlates of Surprisal in
      Language Comprehension: A Neurocomputational Model.
      <i>Frontiers in Psychology 12:110</i></li>
    </ol>
  </div>
</body>
</html>
